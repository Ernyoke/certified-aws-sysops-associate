# Amazon S3

- Amazon S3 allows us to store objects (files) in "buckets" (directories)

## S3 Buckets and Objects

- Buckets must have a globally unique name
- Buckets are defined at the region level
- Buckets do have a naming convention:
    - No uppercase
    - No underscore
    - 3-63 character long names
    - Name should not be an IP
    - Must start with a lower case letter or number
- Objects (files) have a key, which is the full path:
    - s3://my-bucket/my_file.txt
    - s3://my-bucket/my_folder/another_folder/my_file.txt
- They key is composed of the **prefix** + *object name*:
    - s3://my-bucket/ **my_folder/another_folder/** *my_file.txt*
- There is no concept of directories within buckets, although the UI will trick us to think otherwise
- The keys can be very long names which contain slashes ("/")
- Object values are the content of the body
- Max object size is 5TB
- If uploading mor than 5 GB, we must use multi-part upload
- Each object in S3 can have metadata, tags and (optional) version ID

## S3 Versioning

- Versioning should be enabled at the bucket level
- Versioning means: objects uploaded with same key will increment the version
- It is best practice to version files in S3, because:
    - It will protect against unintended deletes (ability to restore version)
    - Provides the ability to rollback a file
- Notes:
    - Any file that is not versioned prior to enabling versioning, will have the version of **null**
    - Suspending versioning does not delete the previous versions of existing files

## S3 Encryption for Objects

- There are 4 methods of encrypting objects in S3:
    - SSE-S3: encrypts S3 objects using keys handled managed by AWS
    - SSE-KMS: leverages AWS KMS to manage encryption keys
    - SSE-C: keys are managed by the users
    - Client Side Encryption

### SSE-S3:

- Encryption using keys handled and managed by AWS
- Objects are encrypted server side
- Uses AES-256 encryption
- In order to upload and set the SSE-S3 encryption, the upload request must have a header `"x-amz-server-side-encryption": "AES256"`

### SSE-KMS

- Encryption using keys handled and managed by KMS
- KMS advantages: user control + audit trail
- Objects are encrypted in the server side
- In order to upload and set the SSE-S3 encryption, the upload request must have a header `"x-amz-server-side-encryption": "aws:kms"`

### SSE-C

- Server-side encryption using the keys provided by the customers
- S3 does not store the encryption key the customers provide
- To upload the data, HTTPS must be used
- Encryption key must be provided in HTTP headers, for every HTTP request

### Client Side Encryption

- Files are encrypted by the customers before uploading them to S3
- Client side libraries can be used to accomplish this, example Amazon S3 Encryption Client
- Customers are responsible for encrypting the data as well
- Encryption keys are fully managed by the customers

## S3 Security

- User based security:
    - IAM policies - which API calls should be allowed for a specific user from IAM console
- Resource based:
    - Bucket policies - bucket wide rules from the S3 console, allows cross account
    - Object Access Control List
    - Bucket Access Control List
- An IAM principal can access an S3 object if:
    - the user IAM permissions allow it or the resource policy allows it
    - there is no explicit DENY

## S3 Bucket Policies

- JSON based documents, which can contain:
    - Resources: buckets and objects
    - Actions: set of API to Allow or Deny
    - Effects: Allow or Deny
    - Principal: the account or user to apply the policy to
- Use S3 bucket for policy to:
    - Grant public access to the bucket
    - Force objects to be encrypted at upload
    - Gran access to another account

## Bucket Settings for Block Public Access

- Block public access to buckets and objects granted through
    - *new* access control list (ACLs)
    - *any* access control list (ACLs)
    - new public bucket or access point policies
- Block public and cross-account access to buckets and objects through any public or access point policies
- These settings were created to prevent company data leaks
- ACL (Access Control List): define read and write permissions at the object level

## S3 Security - Other

- Networking: supports VPC Endpoints
- Logging and audit:
    - S3 Access Logs can be stored in other S3 buckets
    - API calls can be logged in AWS CloudTrail
- Ser Security:
    - MFA Delete
    - Pre-Signed URLs: URLS that are valid only for a limited time

## S3 Websites

- S3 can host static websites and have them accessible on the web
- The website url will be something like this:
    - s3-website dash (-) Region ‐ http://bucket-name.s3-website-Region.amazonaws.com
    - s3-website dot (.) Region ‐ http://bucket-name.s3-website.Region.amazonaws.com
- If we forget to allow public reads in bucket policies, we will get a 403 - Forbidden when accessing the website

## CORS

- An origin is a scheme (protocol), host (domain) and a port
- CORS: Cross-Origin Resource Sharing
- Web browser based mechanism to allow requests to other origins while visiting the main origin
- The request wont ne fulfilled unless the other origin allows for the request, using CORS Headers (ex: `Access-Control-Allow-Origin`, `Access-Control-Allow-Method`)
- If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers
- We can allow for a specific origin or for `*` (all origins)

## S3 Consistency Model

- S3 provides *read after write* consistency for PUTS of new objects
    - As son as a new object is written, we can retrieve it (PUT 200 => GET 200)
    - This is true, except if we did a GET before to see if the object existed (GET 404 => PUT 200 => GET 404)
- S3 provides *eventual consistency* for DELETES and PUTS of existing objects
    - If we read an object after updating it, we might get a the older version (PUT 200 => PUT 200 => GET 200 (which might return the older version))
    - If we delete an object, we might be able to delete it for a short amount of time (DELETE 200 => GET 200)
- There is no way to request strong consistency for S3!

## S3 Advanced Versioning

- S3 Versioning creates a new version each time a file is changed
- That includes when we encrypt a file
- It is a nice ti get protected against unintentional file encryptions (example: ransomware)
- Deleting a file is S3 bucket just adds a delete marker on the versioning
- To delete a bucket, we need to remove all the file and their versions from it

## S3 MFA-Delete

- MFA forces users to generate a code on a device (usually a mobile phone) before doing import operations on S3
- To use MFA-Delete, we have to enable versioning on the S3 bucket
- If MFA-Delete is active, we would require MFA generated codes in order to:
    - Permanently delete an object version
    - Suspend versioning on the bucket
- We don't need MFA codes for:
    - Enabling versioning
    - Listing deleted versions
- Only bucket owner (root account) can enable/disable MFA-Delete!
- MFA-Delete can be enabled currently using the CLI only!

## S3 Default Encryption and Bucket Policies

- The old way of enabling default encryption was to use a bucket policy and refuse any HTTP command without proper headers
- The new way is to use the default encryption option in S3
- Bucket polices are evaluated before the default encryption flag

## S3 Access Logs

- For audit purposes, we may want to log all access to S3 buckets
- Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket
- This data can be analyzed using data analysis tools or Amazon Athena
- **WARNING**: We should not set the logging bucket to be the monitored bucket. This will create a logging loop and the bucket will grow in size exponentially!

## S3 Replication

- Replication can be cross-region (CRR) or same-region (SRR)
- It happens asynchronously
- In order to enable S3 replication, we must have versioning enabled in the source and destination buckets
- Buckets can be in different accounts and must have proper IAM permissions
- CRR - use cases: compliance, lower latency access, replication across accounts
- SRR - use cases: log aggregation, live replication between production and test accounts
- After activating replication, only new objects are replicated
- For delete operations:
    - If we delete without a version ID, S3 adds a delete market which is not replicated
    - If we delete an object with version ID, it deletes the object from the source bucket, operation is not replicated
    - Delete marker replication is an option which can be enabled
- There is no "chaining" of replication: if a bucket 1 has replication in bucket 2, which has replication in bucket 3, then objects in bucket 1 are not replicated in bucket 3

## S3 Pre-Signed URLs

- We can generate pre-signed URLs using SDK or CLI
- A pre-signed URL is valid for a default 3600 seconds, can change timeout with `--expires-in [SECONDS] argument`
- Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET/PUT
- Use cases for pre-signed URLs:
    - Allow only logged-in users to download a premium video from a bucket
    - Allow an even changing list of users to download files by generating URLs dynamically
    - Allow temporarily a user to upload a file to a precise location in a bucket
- Command to generate a presigned url:
    `aws s3 presign s3://bucket-name --region us-east-2 --expires-in 300`

## CloudFront

- It is content delivery network (CDN)
- Improves read performance by caching content at the edge locations
- Can provide DDoS protection, integration with AWS Shield and AWS WAF
- Can expose external HTTPS and can talk to intern HTTPS back-ends

## CloudFront - Origins

- S3 buckets:
    - For distributing files and caching them at the edge
    - Enhanced security with CloudFront Origin Access Identity (OAI)
    - CloudFront can be used as an ingress to upload files
- Custom Origin (HTTP)
    - Application Load Balancer
    - EC2 instance
    - S3 website (must first enable the bucket as a static S3 website)
    - Any HTTP backend

## CloudFront Geo Restriction

- We can restrict who can access our distribution by using:
    - Whitelists: allow our users to access our content only if they are in on of the countries on a list of approved countries
    - Blacklist: prevent our users from accessing our content if they are in one the countries on a blacklist of banned countries

## CloudFront vs S3 CRR

- CloudFront:
    - Uses global edge network
    - Files are cached for a TTL
    - Great for static content which must be available everywhere
- S3 Cross Region Replication:
    - Must be set up for each region we want replication to happen
    - Files are updated in near real-time
    - Great for dynamic content that needs to be available at low-latency in a few regions

## CloudFront Access Logs

- CloudFront access logs: contain every request made to CloudFront
- Access logs are stored in S3

## CloudFront Reports

- It is possible to generate reports on:
    - Cache statistics report
    - Popular object reports
    - Top referrers report
    - Usage report
    - Viewers report
- These reports are based on the data from access logs

## CloudFront Troubleshooting

- CloudFront caches HTTP 4xx and 5xx status codes returned by S3 or the origin server
- 4xx error code indicates the user does not have access to the underlying resources (403) or the object requested does not exist (404)
- 5xx error codes indicate gateway issues